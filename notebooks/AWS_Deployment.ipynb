{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Deployment\n",
    "\n",
    "Here we will take everything above and modularize it to automate training in Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\RaviB\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "MLFLOW_FOLDER = Path(\"mlruns\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"../data/raw_data_2014-2017.csv\"\n",
    "\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading evironment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "mlflow_experiment_name = os.environ[\"MLFLOW_EXPERIMENT_NAME\"]\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/price_history\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads the input data from a CSV file \n",
    "    and does some simple cleaning.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "\n",
    "    raw_train_test_data = pd.concat(raw_data)\n",
    "    raw_train_test_data = raw_train_test_data.drop(columns='Unnamed: 0')\n",
    "\n",
    "    return raw_train_test_data\n",
    "\n",
    "def _get_early_discount_target(df):\n",
    "    \"\"\"Filter for games that actually went on sale.\n",
    "\n",
    "    This function uses a SaleType columns to only consider games that actually went on sale\n",
    "    Then drops that column and creates a new binary one for games that went on sale on release or not\n",
    "    \"\"\"\n",
    "    discounted_games = df[df['SaleType'] == 'went on sale']\n",
    "    discounted_games = discounted_games.drop(columns='SaleType')\n",
    "    discounted_games['DiscountedEarly'] = discounted_games['TimeDelta'].apply(lambda x: 'discounted within 2 days' if x < 3 else 'discounted after 3 days')\n",
    "    return discounted_games\n",
    "\n",
    "def _encoding_multilabel_column(df, feature, frequency_threshold):\n",
    "    #replace labels with spacing with a dash so that they remain one word\n",
    "    #then split into list based on commas\n",
    "    df[feature] = df[feature].apply(lambda x: x.replace(' ', '-').split(','))\n",
    "\n",
    "    all_labels = [label for sublist in df[feature] for label in sublist]\n",
    "    labels_counter = Counter(all_labels)\n",
    "\n",
    "    frequent_cats = {label for label, count in labels_counter.items() if count >= frequency_threshold}\n",
    "    df[f\"Filtered_{feature}\"] = df[feature].apply(lambda x: [label for label in x if label in frequent_cats])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    one_hot_encoded = mlb.fit_transform(df[f\"Filtered_{feature}\"])\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded data\n",
    "    one_hot_df_labels = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "\n",
    "    return one_hot_df_labels\n",
    "\n",
    "def _split_data_discount_on_release(df):\n",
    "    \"\"\"Split the data into train and test for classification.\n",
    "\n",
    "    This function splits the data into training and testing sets\n",
    "    for classification - predicting if a game went on sale within 2 days or not\n",
    "    \"\"\"\n",
    "    # pass in preprocessed_tabular_df as input\n",
    "    y = df['DiscountedEarly']\n",
    "    X = df.drop(columns=['DiscountedEarly', 'TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    label_encoding = {'discounted within 2 days': 0, 'discounted after 3 days': 1}\n",
    "    y = y.apply(lambda x: label_encoding[x])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_encoding\n",
    "\n",
    "def _split_data_discount_after_release(df):\n",
    "    \"\"\"Split the data into train and test for regression.\n",
    "    \n",
    "    This function splits the data into training and testing sets \n",
    "    for predicting how many months until a game goes one sale,\n",
    "    given that it didn't within the first 2 days.\n",
    "    \"\"\"\n",
    "    regression_df = df[df['DiscountedEarly'] == 'discounted after 3 days']\n",
    "    # target for classification task that we don't need anymore after filtering\n",
    "    regression_df = regression_df.drop(columns='DiscountedEarly')\n",
    "\n",
    "    #removing games that went on sale after 6 months for optimal model performance\n",
    "    regression_df['TimeDelta'] = regression_df[['TimeDelta']] // 30 \n",
    "    regression_df = regression_df[regression_df['TimeDelta'] < 6]\n",
    "\n",
    "    # log transform since data has exponential decay\n",
    "    y = np.log1p(regression_df['TimeDelta'])\n",
    "    X = regression_df.drop(columns=['TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "    train_path_name,\n",
    "    test_path_name\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "\n",
    "    train_path_name (str): should be either \"train_clf\" or train_reg\"\n",
    "    test_path_name (str): should be either \"test_clf\" or test_reg\"\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / train_path_name\n",
    "    test_path = Path(base_directory) / test_path_name\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train.to_csv(train_path / f\"{train_path_name}.csv\", header=True, index=False)\n",
    "    test.to_csv(test_path / f\"{test_path_name}.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    discounted_games = _get_early_discount_target(df)\n",
    "\n",
    "    tabular_df = discounted_games[\n",
    "        [\n",
    "        'Achievements', 'Supported languages',\n",
    "        'Mac', 'Linux', 'Categories', 'Tags', \n",
    "        'ReleaseDate', 'TimeDelta', 'DiscountedEarly'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # converting binary columns to ints\n",
    "    tabular_df['Achievements'] = tabular_df['Achievements'].astype(int)\n",
    "    tabular_df['Mac'] = tabular_df['Mac'].astype(int)\n",
    "    tabular_df['Linux'] = tabular_df['Linux'].astype(int)\n",
    "\n",
    "    # converting supported languages to just the number of them\n",
    "    tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(ast.literal_eval)\n",
    "    tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(lambda lst: len(lst))\n",
    "\n",
    "    # converting release date to the month only\n",
    "    tabular_df['ReleaseDate'] = pd.to_datetime(tabular_df['ReleaseDate'])\n",
    "    tabular_df['month'] = tabular_df['ReleaseDate'].dt.month\n",
    "\n",
    "    tabular_df = tabular_df.dropna(subset=['Categories', 'Tags'])\n",
    "    tabular_df = tabular_df.reset_index(drop=True)\n",
    "\n",
    "    one_hot_df_cats = _encoding_multilabel_column(tabular_df, 'Categories', 50)\n",
    "    one_hot_df_tags = _encoding_multilabel_column(tabular_df, 'Tags', 100)\n",
    "\n",
    "    preprocessed_tabular_df = tabular_df.drop(columns=['Categories', 'Tags', 'Filtered_Categories', 'Filtered_Tags', 'ReleaseDate'])\n",
    "    preprocessed_tabular_df = pd.concat([preprocessed_tabular_df, one_hot_df_cats, one_hot_df_tags], axis=1)\n",
    "\n",
    "    X_train_clf, X_test_clf, y_train_clf, y_test_clf, label_encoding = _split_data_discount_on_release(preprocessed_tabular_df)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = _split_data_discount_after_release(preprocessed_tabular_df)\n",
    "\n",
    "    _save_splits(base_directory, X_train_clf, y_train_clf, X_test_clf, y_test_clf, \"train_clf\", \"test_clf\")\n",
    "    _save_splits(base_directory, X_train_reg, y_train_reg, X_test_reg, y_test_reg, \"train_reg\", \"test_reg\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads the input data from a CSV file \n",
    "    and does some simple cleaning.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "\n",
    "    raw_train_test_data = pd.concat(raw_data)\n",
    "    raw_train_test_data = raw_train_test_data.drop(columns='Unnamed: 0')\n",
    "\n",
    "    return raw_train_test_data\n",
    "\n",
    "def _get_early_discount_target(df):\n",
    "    \"\"\"Filter for games that actually went on sale.\n",
    "\n",
    "    This function uses a SaleType columns to only consider games that actually went on sale\n",
    "    Then drops that column and creates a new binary one for games that went on sale on release or not\n",
    "    \"\"\"\n",
    "    discounted_games = df[df['SaleType'] == 'went on sale']\n",
    "    discounted_games = discounted_games.drop(columns='SaleType')\n",
    "    discounted_games['DiscountedEarly'] = discounted_games['TimeDelta'].apply(lambda x: 'discounted within 2 days' if x < 3 else 'discounted after 3 days')\n",
    "    return discounted_games\n",
    "\n",
    "def _encoding_multilabel_column(df, feature, frequency_threshold):\n",
    "    #replace labels with spacing with a dash so that they remain one word\n",
    "    #then split into list based on commas\n",
    "    df[feature] = df[feature].apply(lambda x: x.replace(' ', '-').split(','))\n",
    "\n",
    "    all_labels = [label for sublist in df[feature] for label in sublist]\n",
    "    labels_counter = Counter(all_labels)\n",
    "\n",
    "    frequent_cats = {label for label, count in labels_counter.items() if count >= frequency_threshold}\n",
    "    df[f\"Filtered_{feature}\"] = df[feature].apply(lambda x: [label for label in x if label in frequent_cats])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    one_hot_encoded = mlb.fit_transform(df[f\"Filtered_{feature}\"])\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded data\n",
    "    one_hot_df_labels = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "\n",
    "    return one_hot_df_labels\n",
    "\n",
    "def _split_data_discount_on_release(df):\n",
    "    \"\"\"Split the data into train and test for classification.\n",
    "\n",
    "    This function splits the data into training and testing sets\n",
    "    for classification - predicting if a game went on sale within 2 days or not\n",
    "    \"\"\"\n",
    "    # pass in preprocessed_tabular_df as input\n",
    "    y = df['DiscountedEarly']\n",
    "    X = df.drop(columns=['DiscountedEarly', 'TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    label_encoding = {'discounted within 2 days': 0, 'discounted after 3 days': 1}\n",
    "    y = y.apply(lambda x: label_encoding[x])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_encoding\n",
    "\n",
    "def _split_data_discount_after_release(df):\n",
    "    \"\"\"Split the data into train and test for regression.\n",
    "    \n",
    "    This function splits the data into training and testing sets \n",
    "    for predicting how many months until a game goes one sale,\n",
    "    given that it didn't within the first 2 days.\n",
    "    \"\"\"\n",
    "    regression_df = df[df['DiscountedEarly'] == 'discounted after 3 days']\n",
    "    # target for classification task that we don't need anymore after filtering\n",
    "    regression_df = regression_df.drop(columns='DiscountedEarly')\n",
    "\n",
    "    #removing games that went on sale after 6 months for optimal model performance\n",
    "    regression_df['TimeDelta'] = regression_df[['TimeDelta']] // 30 \n",
    "    regression_df = regression_df[regression_df['TimeDelta'] < 6]\n",
    "\n",
    "    # log transform since data has exponential decay\n",
    "    y = np.log1p(regression_df['TimeDelta'])\n",
    "    X = regression_df.drop(columns=['TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "    train_path_name,\n",
    "    test_path_name\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "\n",
    "    train_path_name (str): should be either \"train_clf\" or train_reg\"\n",
    "    test_path_name (str): should be either \"test_clf\" or test_reg\"\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / train_path_name\n",
    "    test_path = Path(base_directory) / test_path_name\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train.to_csv(train_path / f\"{train_path_name}.csv\", header=False, index=False)\n",
    "    test.to_csv(test_path / f\"{test_path_name}.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw_data_2014-2017.csv')\n",
    "\n",
    "discounted_games = _get_early_discount_target(df)\n",
    "\n",
    "tabular_df = discounted_games[\n",
    "    [\n",
    "    'Achievements', 'Supported languages',\n",
    "    'Mac', 'Linux', 'Categories', 'Tags', \n",
    "    'ReleaseDate', 'TimeDelta', 'DiscountedEarly'\n",
    "    ]\n",
    "]\n",
    "\n",
    "# converting binary columns to ints\n",
    "tabular_df['Achievements'] = tabular_df['Achievements'].astype(int)\n",
    "tabular_df['Mac'] = tabular_df['Mac'].astype(int)\n",
    "tabular_df['Linux'] = tabular_df['Linux'].astype(int)\n",
    "\n",
    "# converting supported languages to just the number of them\n",
    "tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(ast.literal_eval)\n",
    "tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(lambda lst: len(lst))\n",
    "\n",
    "# converting release date to the month only\n",
    "tabular_df['ReleaseDate'] = pd.to_datetime(tabular_df['ReleaseDate'])\n",
    "tabular_df['month'] = tabular_df['ReleaseDate'].dt.month\n",
    "\n",
    "tabular_df = tabular_df.dropna(subset=['Categories', 'Tags'])\n",
    "tabular_df = tabular_df.reset_index(drop=True)\n",
    "\n",
    "one_hot_df_cats = _encoding_multilabel_column(tabular_df, 'Categories', 50)\n",
    "one_hot_df_tags = _encoding_multilabel_column(tabular_df, 'Tags', 100)\n",
    "\n",
    "preprocessed_tabular_df = tabular_df.drop(columns=['Categories', 'Tags', 'Filtered_Categories', 'Filtered_Tags', 'ReleaseDate'])\n",
    "preprocessed_tabular_df = pd.concat([preprocessed_tabular_df, one_hot_df_cats, one_hot_df_tags], axis=1)\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf, label_encoding = _split_data_discount_on_release(preprocessed_tabular_df)\n",
    "#X_train_reg, X_test_reg, y_train_reg, y_test_reg = _split_data_discount_after_release(preprocessed_tabular_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540, 216)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10159, 217)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat([X_train_clf, y_train_clf], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train_clf, y_train_clf], axis=1).to_csv(\"../data/train_clf.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 6720, 0: 3438})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clf = pd.read_csv(\"../data/train_clf.csv\")\n",
    "y_train_clf_saved = train_clf[train_clf.columns[-1]]\n",
    "X_train_clf_saved = train_clf.drop(train_clf.columns[-1], axis=1)\n",
    "Counter(y_train_clf_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10158, 216)\n",
      "(10158,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train_clf_saved.shape)\n",
    "print('y_train shape:', y_train_clf_saved.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure caching and a sagemaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check what version we are using\n",
    "import xgboost as xgb\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "import xgboost as xgb\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "\n",
    "config = {\n",
    "    \"session\": pipeline_session,\n",
    "    #\"instance_type\": \"ml.t3.medium\",\n",
    "    \"instance_type\": \"ml.m5.xlarge\",\n",
    "    \"image\": None,\n",
    "}\n",
    "\n",
    "# These specific settings refer to the SageMaker\n",
    "# xgboost container we'll use, for some reason the version uses a dash\n",
    "config[\"framework_version\"] = \"1.7-1\"\n",
    "config[\"py_version\"] = \"py3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n",
    "    # If you haven't requested a quota increase yet, you can use an\n",
    "    # `ml.t3.medium` instance type instead. This will work out of the box, but\n",
    "    # the Processing Job will take significantly longer than it should have.\n",
    "    # To get access to `ml.m5.xlarge` instances, you can request a quota\n",
    "    # increase under the Service Quotas section in your AWS account.\n",
    "    #instance_type=\"ml.t3.medium\",\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterString(name='dataset_location', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://steamgames/price_history/data')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'processing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train_clf\",\n",
    "                source=\"/opt/ml/processing/train_clf\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train_clf\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test_clf\",\n",
    "                source=\"/opt/ml/processing/test_clf\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test_clf\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train_reg\",\n",
    "                source=\"/opt/ml/processing/train_reg\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train_reg\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test_reg\",\n",
    "                source=\"/opt/ml/processing/test_reg\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test_reg\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the sagemaker pipeline, or update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '86a4cecb-5050-47b6-a4e9-83c1bd3c54e0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '86a4cecb-5050-47b6-a4e9-83c1bd3c54e0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Fri, 31 May 2024 14:45:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"training\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/train_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/train_script.py\n",
    "# | filename: train_script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from collections import Counter\n",
    "\n",
    "def train(\n",
    "    model_directory,\n",
    "    train_path,\n",
    "    test_path,\n",
    "    #pipeline_path,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    "):\n",
    "    print('train_clf path:', Path(train_path) / \"train_clf.csv\")\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train_clf.csv\")\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test_clf.csv\")\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "\n",
    "    print('y_train unique value counts:', Counter(y_train))\n",
    "    print('y_test unique value counts:', Counter(y_test))\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': max_depth,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    model_filepath = (Path(model_directory) / \"discount_on_release.xgboost_clf\")\n",
    "\n",
    "    model.save(model_filepath)\n",
    "\n",
    "# def train_reg(\n",
    "#     model_directory,\n",
    "#     train_reg_path,\n",
    "#     test_reg_path,\n",
    "#     pipeline_path,\n",
    "#     learning_rate=0.05,\n",
    "#     min_child_weight=5,\n",
    "#     n_estimators=50,\n",
    "#     reg_alpha=0.01,\n",
    "#     reg_lambda=1.5\n",
    "# ):\n",
    "#     X_train = pd.read_csv(Path(train_reg_path) / \"train_reg.csv\")\n",
    "#     y_train = X_train[X_train.columns[-1]]\n",
    "#     X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "#     X_test = pd.read_csv(Path(test_reg_path) / \"test_reg.csv\")\n",
    "#     y_test = X_test[X_test.columns[-1]]\n",
    "#     X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "#     params = {\n",
    "#         'learning_rate': learning_rate,\n",
    "#         'min_child_weight': min_child_weight,\n",
    "#         'n_estimators': n_estimators,\n",
    "#         'reg_alpha': reg_alpha,\n",
    "#         'reg_lambda': reg_lambda,\n",
    "#         'objective': 'reg:squarederror',\n",
    "#     }\n",
    "\n",
    "#     model = xgb.XGBRegressor(**params)\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#     model_filepath = (Path(model_directory) / \"time_until_discount.xgboost_reg\")\n",
    "\n",
    "#     with tarfile.open(Path(pipeline_path) / \"model.tar.gz\", \"r:gz\") as tar:\n",
    "#         tar.extractall(model_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to\n",
    "    # the entry point as script arguments.\n",
    "    # default values came from a local grid search run\n",
    "    print(f\"SM_MODEL_DIR: {os.environ.get('SM_MODEL_DIR')}\")\n",
    "    print(f\"SM_CHANNEL_TRAIN: {os.environ.get('SM_CHANNEL_TRAIN_CLF')}\")\n",
    "    print(f\"SM_CHANNEL_TEST: {os.environ.get('SM_CHANNEL_TEST_CLF')}\")\n",
    "    #print(f\"SM_CHANNEL_PIPELINE: {os.environ.get('SM_CHANNEL_PIPELINE')}\")\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # for the classifier model\n",
    "    parser.add_argument(\"--learning_rate_clf\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--max_depth_clf\", type=int, default=3)\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN_CLF'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST_CLF'])\n",
    "    #for the regression model\n",
    "    # parser.add_argument(\"--learning_rate_reg\", type=float, default=0.05)\n",
    "    # parser.add_argument(\"--min_child_weight_reg\", type=int, default=5)\n",
    "    # parser.add_argument(\"--n_estimators_reg\", type=int, default=50)\n",
    "    # parser.add_argument(\"--reg_alpha_reg\", type=float, default=0.01)\n",
    "    # parser.add_argument(\"--reg_lambda_reg\", type=float, default=1.5)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    training_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", {}))\n",
    "    job_name = training_env.get(\"job_name\", None) if training_env else None\n",
    "\n",
    "    \n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model.\n",
    "        # SageMaker will create a model.tar.gz file with anything\n",
    "        # inside this directory when the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        # SageMaker creates one channel for each one of the inputs\n",
    "        # to the Training Step.\n",
    "        train_path=args.train,\n",
    "        test_path=args.test,\n",
    "        #pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n",
    "        learning_rate=args.learning_rate_clf,\n",
    "        max_depth=args.max_depth_clf,\n",
    "    )\n",
    "\n",
    "    # train_reg(\n",
    "    #     model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "    #     train_reg_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "    #     test_reg_path=os.environ[\"SM_CHANNEL_TEST\"],\n",
    "    #     pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n",
    "    #     learning_rate=args.learning_rate_reg,\n",
    "    #     min_child_weight=args.min_child_weight_reg,\n",
    "    #     n_estimators=args.n_estimators_reg,\n",
    "    #     reg_alpha=args.reg_alpha_reg,\n",
    "    #     reg_lambda=args.reg_lambda_reg\n",
    "    # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=\"train_script.py\",\n",
    "    source_dir=f\"{(CODE_FOLDER / 'training').as_posix()}\",\n",
    "    hyperparameters={\n",
    "        \"learning_rate_clf\": 0.1,\n",
    "        \"max_depth_clf\": 3,\n",
    "        # \"learning_rate_reg\": 0.05,\n",
    "        # \"min_child_weight_reg\": 5,\n",
    "        # \"n_estimators_reg\": 50,\n",
    "        # \"reg_alpha_reg\": 0.01,\n",
    "        # \"reg_lambda_reg\": 1.5,\n",
    "    },\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_step(estimator):\n",
    "    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n",
    "    return TrainingStep(\n",
    "        name=\"train-model\",\n",
    "        step_args=estimator.fit(\n",
    "            inputs={\n",
    "                \"train_clf\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"train_clf\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test_clf\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test_clf\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                # \"train_reg\": TrainingInput(\n",
    "                #     s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                #         \"train_reg\"\n",
    "                #     ].S3Output.S3Uri,\n",
    "                #     content_type=\"text/csv\",\n",
    "                # ),\n",
    "                # \"test_reg\": TrainingInput(\n",
    "                #     s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                #         \"test_reg\"\n",
    "                #     ].S3Output.S3Uri,\n",
    "                #     content_type=\"text/csv\",\n",
    "                # ),\n",
    "            },\n",
    "        ),\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "train_model_step = create_training_step(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'df12ea89-6dda-4e00-a078-360e778b793c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'df12ea89-6dda-4e00-a078-360e778b793c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Fri, 31 May 2024 15:00:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "train_pipeline = Pipeline(\n",
    "    name=\"train-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "train_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline/execution/fhgca76hchys', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001415F477040>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SteamSales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
