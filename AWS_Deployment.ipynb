{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Deployment\n",
    "\n",
    "Here we will take everything above and modularize it to automate training in Sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\RaviB\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "MLFLOW_FOLDER = Path(\"mlruns\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"../data/raw_data_2014-2017.csv\"\n",
    "\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading evironment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "mlflow_experiment_name = os.environ[\"MLFLOW_EXPERIMENT_NAME\"]\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/price_history\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import ast\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads the input data from a CSV file \n",
    "    and does some simple cleaning.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "\n",
    "    raw_train_test_data = pd.concat(raw_data)\n",
    "    raw_train_test_data = raw_train_test_data.drop(columns='Unnamed: 0')\n",
    "\n",
    "    return raw_train_test_data\n",
    "\n",
    "def _get_early_discount_target(df):\n",
    "    \"\"\"Filter for games that actually went on sale.\n",
    "\n",
    "    This function uses a SaleType columns to only consider games that actually went on sale\n",
    "    Then drops that column and creates a new binary one for games that went on sale on release or not\n",
    "    \"\"\"\n",
    "    discounted_games = df[df['SaleType'] == 'went on sale']\n",
    "    discounted_games = discounted_games.drop(columns='SaleType')\n",
    "    discounted_games['DiscountedEarly'] = discounted_games['TimeDelta'].apply(lambda x: 'discounted within 2 days' if x < 3 else 'discounted after 3 days')\n",
    "    return discounted_games\n",
    "\n",
    "def _encoding_multilabel_column(df, feature, frequency_threshold):\n",
    "    #replace labels with spacing with a dash so that they remain one word\n",
    "    #then split into list based on commas\n",
    "    df[feature] = df[feature].apply(lambda x: x.replace(' ', '-').split(','))\n",
    "\n",
    "    all_labels = [label for sublist in df[feature] for label in sublist]\n",
    "    labels_counter = Counter(all_labels)\n",
    "\n",
    "    frequent_cats = {label for label, count in labels_counter.items() if count >= frequency_threshold}\n",
    "    df[f\"Filtered_{feature}\"] = df[feature].apply(lambda x: [label for label in x if label in frequent_cats])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    mlb.fit(df[f\"Filtered_{feature}\"])\n",
    "    one_hot_encoded = mlb.transform(df[f\"Filtered_{feature}\"])\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded data\n",
    "    one_hot_df_labels = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "\n",
    "    return one_hot_df_labels, mlb\n",
    "\n",
    "def _split_data_discount_on_release(df):\n",
    "    \"\"\"Split the data into train and test for classification.\n",
    "\n",
    "    This function splits the data into training and testing sets\n",
    "    for classification - predicting if a game went on sale within 2 days or not\n",
    "    \"\"\"\n",
    "    # pass in preprocessed_tabular_df as input\n",
    "    y = df['DiscountedEarly']\n",
    "    X = df.drop(columns=['DiscountedEarly', 'TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    label_encoding = {'discounted within 2 days': 0, 'discounted after 3 days': 1}\n",
    "    y = y.apply(lambda x: label_encoding[x])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_encoding\n",
    "\n",
    "def _split_data_discount_after_release(df):\n",
    "    \"\"\"Split the data into train and test for regression.\n",
    "    \n",
    "    This function splits the data into training and testing sets \n",
    "    for predicting how many months until a game goes one sale,\n",
    "    given that it didn't within the first 2 days.\n",
    "    \"\"\"\n",
    "    regression_df = df[df['DiscountedEarly'] == 'discounted after 3 days']\n",
    "    # target for classification task that we don't need anymore after filtering\n",
    "    regression_df = regression_df.drop(columns='DiscountedEarly')\n",
    "\n",
    "    #removing games that went on sale after 6 months for optimal model performance\n",
    "    regression_df['TimeDelta'] = regression_df[['TimeDelta']] // 30 \n",
    "    regression_df = regression_df[regression_df['TimeDelta'] < 6]\n",
    "\n",
    "    # log transform since data has exponential decay\n",
    "    y = np.log1p(regression_df['TimeDelta'])\n",
    "    X = regression_df.drop(columns=['TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "    train_path_name,\n",
    "    test_path_name\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "\n",
    "    train_path_name (str): should be either \"train_clf\" or train_reg\"\n",
    "    test_path_name (str): should be either \"test_clf\" or test_reg\"\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / train_path_name\n",
    "    test_path = Path(base_directory) / test_path_name\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train.to_csv(train_path / f\"{train_path_name}.csv\", header=True, index=False)\n",
    "    test.to_csv(test_path / f\"{test_path_name}.csv\", header=True, index=False)\n",
    "\n",
    "def _save_mlb(base_directory, mlb, mlb_name):\n",
    "    \"\"\"Save the MultiLabelBinarizer object locally.\"\"\"\n",
    "    file_path = Path(base_directory) / mlb_name\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(mlb, f)\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    discounted_games = _get_early_discount_target(df)\n",
    "\n",
    "    tabular_df = discounted_games[\n",
    "        [\n",
    "        'Achievements', 'Supported languages',\n",
    "        'Mac', 'Linux', 'Categories', 'Tags', \n",
    "        'ReleaseDate', 'TimeDelta', 'DiscountedEarly'\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # converting binary columns to ints\n",
    "    tabular_df['Achievements'] = tabular_df['Achievements'].astype(int)\n",
    "    tabular_df['Mac'] = tabular_df['Mac'].astype(int)\n",
    "    tabular_df['Linux'] = tabular_df['Linux'].astype(int)\n",
    "\n",
    "    # converting supported languages to just the number of them\n",
    "    tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(ast.literal_eval)\n",
    "    tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(lambda lst: len(lst))\n",
    "\n",
    "    # converting release date to the month only\n",
    "    tabular_df['ReleaseDate'] = pd.to_datetime(tabular_df['ReleaseDate'])\n",
    "    tabular_df['month'] = tabular_df['ReleaseDate'].dt.month\n",
    "\n",
    "    tabular_df = tabular_df.dropna(subset=['Categories', 'Tags'])\n",
    "    tabular_df = tabular_df.reset_index(drop=True)\n",
    "\n",
    "    one_hot_df_cats, mlb_cats = _encoding_multilabel_column(tabular_df, 'Categories', 50)\n",
    "    one_hot_df_tags, mlb_tags = _encoding_multilabel_column(tabular_df, 'Tags', 100)\n",
    "\n",
    "    preprocessed_tabular_df = tabular_df.drop(columns=['Categories', 'Tags', 'Filtered_Categories', 'Filtered_Tags', 'ReleaseDate'])\n",
    "    preprocessed_tabular_df = pd.concat([preprocessed_tabular_df, one_hot_df_cats, one_hot_df_tags], axis=1)\n",
    "\n",
    "    X_train_clf, X_test_clf, y_train_clf, y_test_clf, label_encoding = _split_data_discount_on_release(preprocessed_tabular_df)\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = _split_data_discount_after_release(preprocessed_tabular_df)\n",
    "\n",
    "    _save_mlb(base_directory, mlb_cats, \"mlb_cats.pkl\")\n",
    "    _save_mlb(base_directory, mlb_tags, \"mlb_tags.pkl\")\n",
    "    _save_splits(base_directory, X_train_clf, y_train_clf, X_test_clf, y_test_clf, \"train_clf\", \"test_clf\")\n",
    "    _save_splits(base_directory, X_train_reg, y_train_reg, X_test_reg, y_test_reg, \"train_reg\", \"test_reg\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Preprocessing Functions Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['Achievements'] = tabular_df['Achievements'].astype(int)\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['Mac'] = tabular_df['Mac'].astype(int)\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['Linux'] = tabular_df['Linux'].astype(int)\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(ast.literal_eval)\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(lambda lst: len(lst))\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['ReleaseDate'] = pd.to_datetime(tabular_df['ReleaseDate'])\n",
      "C:\\Users\\RaviB\\AppData\\Local\\Temp\\ipykernel_7632\\4035918349.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tabular_df['month'] = tabular_df['ReleaseDate'].dt.month\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads the input data from a CSV file \n",
    "    and does some simple cleaning.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "\n",
    "    raw_train_test_data = pd.concat(raw_data)\n",
    "    raw_train_test_data = raw_train_test_data.drop(columns='Unnamed: 0')\n",
    "\n",
    "    return raw_train_test_data\n",
    "\n",
    "def _get_early_discount_target(df):\n",
    "    \"\"\"Filter for games that actually went on sale.\n",
    "\n",
    "    This function uses a SaleType columns to only consider games that actually went on sale\n",
    "    Then drops that column and creates a new binary one for games that went on sale on release or not\n",
    "    \"\"\"\n",
    "    discounted_games = df[df['SaleType'] == 'went on sale']\n",
    "    discounted_games = discounted_games.drop(columns='SaleType')\n",
    "    discounted_games['DiscountedEarly'] = discounted_games['TimeDelta'].apply(lambda x: 'discounted within 2 days' if x < 3 else 'discounted after 3 days')\n",
    "    return discounted_games\n",
    "\n",
    "def _encoding_multilabel_column(df, feature, frequency_threshold):\n",
    "    #replace labels with spacing with a dash so that they remain one word\n",
    "    #then split into list based on commas\n",
    "    df[feature] = df[feature].apply(lambda x: x.replace(' ', '-').split(','))\n",
    "\n",
    "    all_labels = [label for sublist in df[feature] for label in sublist]\n",
    "    labels_counter = Counter(all_labels)\n",
    "\n",
    "    frequent_cats = {label for label, count in labels_counter.items() if count >= frequency_threshold}\n",
    "    df[f\"Filtered_{feature}\"] = df[feature].apply(lambda x: [label for label in x if label in frequent_cats])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    one_hot_encoded = mlb.fit_transform(df[f\"Filtered_{feature}\"])\n",
    "\n",
    "    # Create a DataFrame with the one-hot encoded data\n",
    "    one_hot_df_labels = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n",
    "\n",
    "    return one_hot_df_labels\n",
    "\n",
    "def _split_data_discount_on_release(df):\n",
    "    \"\"\"Split the data into train and test for classification.\n",
    "\n",
    "    This function splits the data into training and testing sets\n",
    "    for classification - predicting if a game went on sale within 2 days or not\n",
    "    \"\"\"\n",
    "    # pass in preprocessed_tabular_df as input\n",
    "    y = df['DiscountedEarly']\n",
    "    X = df.drop(columns=['DiscountedEarly', 'TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    label_encoding = {'discounted within 2 days': 0, 'discounted after 3 days': 1}\n",
    "    y = y.apply(lambda x: label_encoding[x])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, label_encoding\n",
    "\n",
    "def _split_data_discount_after_release(df):\n",
    "    \"\"\"Split the data into train and test for regression.\n",
    "    \n",
    "    This function splits the data into training and testing sets \n",
    "    for predicting how many months until a game goes one sale,\n",
    "    given that it didn't within the first 2 days.\n",
    "    \"\"\"\n",
    "    regression_df = df[df['DiscountedEarly'] == 'discounted after 3 days']\n",
    "    # target for classification task that we don't need anymore after filtering\n",
    "    regression_df = regression_df.drop(columns='DiscountedEarly')\n",
    "\n",
    "    #removing games that went on sale after 6 months for optimal model performance\n",
    "    regression_df['TimeDelta'] = regression_df[['TimeDelta']] // 30 \n",
    "    regression_df = regression_df[regression_df['TimeDelta'] < 6]\n",
    "\n",
    "    # log transform since data has exponential decay\n",
    "    y = np.log1p(regression_df['TimeDelta'])\n",
    "    X = regression_df.drop(columns=['TimeDelta'])\n",
    "\n",
    "    # duplicate columns to manually drop\n",
    "    X = X.drop(columns=['Co-op', 'PvP'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "    train_path_name,\n",
    "    test_path_name\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "\n",
    "    train_path_name (str): should be either \"train_clf\" or train_reg\"\n",
    "    test_path_name (str): should be either \"test_clf\" or test_reg\"\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / train_path_name\n",
    "    test_path = Path(base_directory) / test_path_name\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train.to_csv(train_path / f\"{train_path_name}.csv\", header=False, index=False)\n",
    "    test.to_csv(test_path / f\"{test_path_name}.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw_data_2014-2017.csv')\n",
    "\n",
    "discounted_games = _get_early_discount_target(df)\n",
    "\n",
    "tabular_df = discounted_games[\n",
    "    [\n",
    "    'Achievements', 'Supported languages',\n",
    "    'Mac', 'Linux', 'Categories', 'Tags', \n",
    "    'ReleaseDate', 'TimeDelta', 'DiscountedEarly'\n",
    "    ]\n",
    "]\n",
    "\n",
    "# converting binary columns to ints\n",
    "tabular_df['Achievements'] = tabular_df['Achievements'].astype(int)\n",
    "tabular_df['Mac'] = tabular_df['Mac'].astype(int)\n",
    "tabular_df['Linux'] = tabular_df['Linux'].astype(int)\n",
    "\n",
    "# converting supported languages to just the number of them\n",
    "tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(ast.literal_eval)\n",
    "tabular_df['Supported languages'] = tabular_df['Supported languages'].apply(lambda lst: len(lst))\n",
    "\n",
    "# converting release date to the month only\n",
    "tabular_df['ReleaseDate'] = pd.to_datetime(tabular_df['ReleaseDate'])\n",
    "tabular_df['month'] = tabular_df['ReleaseDate'].dt.month\n",
    "\n",
    "tabular_df = tabular_df.dropna(subset=['Categories', 'Tags'])\n",
    "tabular_df = tabular_df.reset_index(drop=True)\n",
    "\n",
    "one_hot_df_cats = _encoding_multilabel_column(tabular_df, 'Categories', 50)\n",
    "one_hot_df_tags = _encoding_multilabel_column(tabular_df, 'Tags', 100)\n",
    "\n",
    "preprocessed_tabular_df = tabular_df.drop(columns=['Categories', 'Tags', 'Filtered_Categories', 'Filtered_Tags', 'ReleaseDate'])\n",
    "preprocessed_tabular_df = pd.concat([preprocessed_tabular_df, one_hot_df_cats, one_hot_df_tags], axis=1)\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf, label_encoding = _split_data_discount_on_release(preprocessed_tabular_df)\n",
    "#X_train_reg, X_test_reg, y_train_reg, y_test_reg = _split_data_discount_after_release(preprocessed_tabular_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540, 216)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_clf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10159, 217)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat([X_train_clf, y_train_clf], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train_clf, y_train_clf], axis=1).to_csv(\"../data/train_clf.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 6720, 0: 3438})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clf = pd.read_csv(\"../data/train_clf.csv\")\n",
    "y_train_clf_saved = train_clf[train_clf.columns[-1]]\n",
    "X_train_clf_saved = train_clf.drop(train_clf.columns[-1], axis=1)\n",
    "Counter(y_train_clf_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10158, 216)\n",
      "y_train shape: (10158,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train_clf_saved.shape)\n",
    "print('y_train shape:', y_train_clf_saved.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure caching and a sagemaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check what version we are using\n",
    "import xgboost as xgb\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "import xgboost as xgb\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "\n",
    "config = {\n",
    "    \"session\": pipeline_session,\n",
    "    #\"instance_type\": \"ml.t3.medium\",\n",
    "    \"instance_type\": \"ml.m5.xlarge\",\n",
    "    \"image\": None,\n",
    "}\n",
    "\n",
    "# These specific settings refer to the SageMaker\n",
    "# xgboost container we'll use, for some reason the version uses a dash\n",
    "config[\"framework_version\"] = \"1.7-1\"\n",
    "config[\"py_version\"] = \"py3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n",
    "    # If you haven't requested a quota increase yet, you can use an\n",
    "    # `ml.t3.medium` instance type instead. This will work out of the box, but\n",
    "    # the Processing Job will take significantly longer than it should have.\n",
    "    # To get access to `ml.m5.xlarge` instances, you can request a quota\n",
    "    # increase under the Service Quotas section in your AWS account.\n",
    "    #instance_type=\"ml.t3.medium\",\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterString(name='dataset_location', parameter_type=<ParameterTypeEnum.STRING: 'String'>, default_value='s3://steamgames/price_history/data')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'processing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train_clf\",\n",
    "                source=\"/opt/ml/processing/train_clf\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train_clf\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test_clf\",\n",
    "                source=\"/opt/ml/processing/test_clf\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test_clf\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train_reg\",\n",
    "                source=\"/opt/ml/processing/train_reg\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train_reg\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test_reg\",\n",
    "                source=\"/opt/ml/processing/test_reg\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test_reg\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"mlb_cats\",\n",
    "                source=\"/opt/ml/processing/mlb_cats\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/mlb_cats\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"mlb_tags\",\n",
    "                source=\"/opt/ml/processing/mlb_tags\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/mlb_tags\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the sagemaker pipeline, or update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'b1aeb716-5763-44e6-a819-af385ddf76d8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b1aeb716-5763-44e6-a819-af385ddf76d8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Sat, 08 Jun 2024 15:23:24 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"training\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/train_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/train_script.py\n",
    "# | filename: train_script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "def save_metrics(metrics, model_directory, model_name):\n",
    "    metrics_filepath = Path(model_directory) / f\"{model_name}_metrics.json\"\n",
    "    with open(metrics_filepath, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "def train(\n",
    "    model_directory,\n",
    "    train_path,\n",
    "    test_path,\n",
    "    #pipeline_path,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3\n",
    "):\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train_clf.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test_clf.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': max_depth,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    save_metrics(metrics, model_directory, \"discount_on_release-xgboost\")\n",
    "\n",
    "    model_filepath = (Path(model_directory) / \"discount_on_release-xgboost\")\n",
    "\n",
    "    model.save_model(model_filepath)\n",
    "    #pkl.dump(model, open(model_filepath, 'wb'))\n",
    "\n",
    "def train_reg(\n",
    "    model_directory,\n",
    "    train_reg_path,\n",
    "    test_reg_path,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=5,\n",
    "    n_estimators=50,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=1.5\n",
    "):\n",
    "    X_train = pd.read_csv(Path(train_reg_path) / \"train_reg.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_test = pd.read_csv(Path(test_reg_path) / \"test_reg.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'n_estimators': n_estimators,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        'objective': 'reg:squarederror',\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'r2_score': r2\n",
    "    }\n",
    "\n",
    "    save_metrics(metrics, model_directory, \"time_until_discount-xgboost\")\n",
    "\n",
    "    model_filepath = (Path(model_directory) / \"time_until_discount-xgboost\")\n",
    "\n",
    "    model.save_model(model_filepath)\n",
    "    #pkl.dump(model, open(model_filepath, 'wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to\n",
    "    # the entry point as script arguments.\n",
    "    # default values came from a local grid search run\n",
    "    print(f\"SM_MODEL_DIR: {os.environ.get('SM_MODEL_DIR')}\")\n",
    "    print(f\"SM_CHANNEL_TRAIN: {os.environ.get('SM_CHANNEL_TRAIN_CLF')}\")\n",
    "    print(f\"SM_CHANNEL_TEST: {os.environ.get('SM_CHANNEL_TEST_CLF')}\")\n",
    "    #print(f\"SM_CHANNEL_PIPELINE: {os.environ.get('SM_CHANNEL_PIPELINE')}\")\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # for the classifier model\n",
    "    parser.add_argument(\"--learning_rate_clf\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--max_depth_clf\", type=int, default=3)\n",
    "    #for the regression model\n",
    "    parser.add_argument(\"--learning_rate_reg\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--min_child_weight_reg\", type=int, default=5)\n",
    "    parser.add_argument(\"--n_estimators_reg\", type=int, default=50)\n",
    "    parser.add_argument(\"--reg_alpha_reg\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--reg_lambda_reg\", type=float, default=1.5)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    training_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", {}))\n",
    "    job_name = training_env.get(\"job_name\", None) if training_env else None\n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model.\n",
    "        # SageMaker will create a model.tar.gz file with anything\n",
    "        # inside this directory when the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        # SageMaker creates one channel for each one of the inputs\n",
    "        # to the Training Step.\n",
    "        train_path=os.environ['SM_CHANNEL_TRAIN_CLF'],\n",
    "        test_path=os.environ['SM_CHANNEL_TEST_CLF'],\n",
    "        learning_rate=args.learning_rate_clf,\n",
    "        max_depth=args.max_depth_clf,\n",
    "    )\n",
    "\n",
    "    train_reg(\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        train_reg_path=os.environ[\"SM_CHANNEL_TRAIN_REG\"],\n",
    "        test_reg_path=os.environ[\"SM_CHANNEL_TEST_REG\"],\n",
    "        learning_rate=args.learning_rate_reg,\n",
    "        min_child_weight=args.min_child_weight_reg,\n",
    "        n_estimators=args.n_estimators_reg,\n",
    "        reg_alpha=args.reg_alpha_reg,\n",
    "        reg_lambda=args.reg_lambda_reg\n",
    "    )\n",
    "\n",
    "    model_dir = Path(os.environ[\"SM_MODEL_DIR\"])\n",
    "    tar_path = model_dir / \"model.tar.gz\"\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        tar.add(model_dir / \"discount_on_release-xgboost\", arcname=\"discount_on_release-xgboost\")\n",
    "        tar.add(model_dir / \"time_until_discount-xgboost\", arcname=\"time_until_discount-xgboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=\"train_script.py\",\n",
    "    source_dir=f\"{(CODE_FOLDER / 'training').as_posix()}\",\n",
    "    hyperparameters={\n",
    "        \"learning_rate_clf\": 0.1,\n",
    "        \"max_depth_clf\": 3,\n",
    "        \"learning_rate_reg\": 0.05,\n",
    "        \"min_child_weight_reg\": 5,\n",
    "        \"n_estimators_reg\": 50,\n",
    "        \"reg_alpha_reg\": 0.01,\n",
    "        \"reg_lambda_reg\": 1.5,\n",
    "    },\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_step(estimator):\n",
    "    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n",
    "    return TrainingStep(\n",
    "        name=\"train-model\",\n",
    "        step_args=estimator.fit(\n",
    "            inputs={\n",
    "                \"train_clf\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"train_clf\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test_clf\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test_clf\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"train_reg\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"train_reg\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test_reg\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test_reg\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "train_model_step = create_training_step(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'e556a41e-881b-489d-85db-247c236fca04',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e556a41e-881b-489d-85db-247c236fca04',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Sat, 08 Jun 2024 16:18:44 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "train_pipeline = Pipeline(\n",
    "    name=\"train-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")\n",
    "\n",
    "train_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline/execution/2inc2b2gwi7a', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000002CBA046A530>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't forget to change pipeline name if needed\n",
    "train_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = os.environ[\"BUCKET\"]\n",
    "mlb_cats_key = 'mlb_cats.pkl'\n",
    "mlb_tags_key = 'mlb_tags.pkl'\n",
    "\n",
    "local_mlb_cats_path = Path(\"/opt/ml/processing\") / mlb_cats_key\n",
    "local_mlb_tags_path = 'mlb_tags.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (404) when calling the HeadObject operation: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#s3.download_file(bucket_name, mlb_tags_key, local_mlb_tags_path)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_cats_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_mlb_cats_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\boto3\\s3\\inject.py:192\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\boto3\\s3\\transfer.py:405\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[1;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[0;32m    401\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    402\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[0;32m    403\u001b[0m )\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\s3transfer\\futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\s3transfer\\futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\s3transfer\\tasks.py:269\u001b[0m, in \u001b[0;36mSubmissionTask._main\u001b[1;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mset_status_to_running()\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# transfer.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit(transfer_future\u001b[38;5;241m=\u001b[39mtransfer_future, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# If there was an exception raised during the submission of task\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# Set the exception, that caused the process to fail.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\s3transfer\\download.py:354\u001b[0m, in \u001b[0;36mDownloadSubmissionTask._submit\u001b[1;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m:param client: The client associated with the transfer manager\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    downloading streams\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# If a size was not provided figure out the size for the\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m# user.\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mhead_object(\n\u001b[0;32m    355\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mbucket,\n\u001b[0;32m    356\u001b[0m         Key\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mextra_args,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    359\u001b[0m     transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprovide_transfer_size(\n\u001b[0;32m    360\u001b[0m         response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    363\u001b[0m download_output_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_download_output_manager_cls(\n\u001b[0;32m    364\u001b[0m     transfer_future, osutil\n\u001b[0;32m    365\u001b[0m )(osutil, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator, io_executor)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\botocore\\client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\SteamSales\\lib\\site-packages\\botocore\\client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
     ]
    }
   ],
   "source": [
    "#s3.download_file(bucket_name, mlb_tags_key, local_mlb_tags_path)\n",
    "s3.download_file(bucket_name, mlb_cats_key, local_mlb_cats_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SteamSales",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
